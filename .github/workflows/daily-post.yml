name: Daily Social Media Post

on:
  schedule:
    # Post at exact times with time slot window logic
    # 9:00 AM EST = 14:00 UTC (EST) / 13:00 UTC (EDT)
    # 11:00 AM EST = 16:00 UTC (EST) / 15:00 UTC (EDT)  
    # 1:00 PM EST = 18:00 UTC (EST) / 17:00 UTC (EDT)
    # 3:00 PM EST = 20:00 UTC (EST) / 19:00 UTC (EDT)
    
    # Winter schedule (EST - Nov-Mar)
    - cron: '0 14 * 11-12,1-3 1-5'  # 9am EST, Mon-Fri
    - cron: '0 16 * 11-12,1-3 1-5'  # 11am EST, Mon-Fri
    - cron: '0 18 * 11-12,1-3 1-5'  # 1pm EST, Mon-Fri
    - cron: '0 20 * 11-12,1-3 1-5'  # 3pm EST, Mon-Fri
    
    # Summer schedule (EDT - Mar-Nov)  
    - cron: '0 13 * 4-10 1-5'  # 9am EDT, Mon-Fri
    - cron: '0 15 * 4-10 1-5'  # 11am EDT, Mon-Fri
    - cron: '0 17 * 4-10 1-5'  # 1pm EDT, Mon-Fri
    - cron: '0 19 * 4-10 1-5'  # 3pm EDT, Mon-Fri
  
  # Allow manual trigger
  workflow_dispatch:

jobs:
  post-daily:
    runs-on: ubuntu-latest
    permissions:
      actions: write
      contents: read
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create data directory
      run: mkdir -p data
    
    - name: Find latest database from any workflow
      id: get_latest_db
      uses: actions/github-script@v7
      with:
        script: |
          // Get runs from all three workflows
          const workflows = ['fetch-posts.yml', 'daily-post.yml', 'clear-unposted.yml'];
          const allRuns = [];
          
          for (const workflow of workflows) {
            try {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: workflow,
                branch: 'main',
                per_page: 10,
                status: 'success'
              });
              if (runs.data.workflow_runs) {
                allRuns.push(...runs.data.workflow_runs.map(run => ({
                  ...run,
                  workflow_name: workflow
                })));
              }
            } catch (error) {
              console.log(`Error fetching runs for ${workflow}:`, error.message);
            }
          }
          
          // Sort by creation time (newest first)
          allRuns.sort((a, b) => new Date(b.created_at) - new Date(a.created_at));
          
          const latestRun = allRuns[0];
          if (latestRun) {
            console.log(`Latest database source: ${latestRun.workflow_name} (run ${latestRun.id})`);
            core.setOutput('run_id', String(latestRun.id));
            core.setOutput('workflow_name', latestRun.workflow_name);
            core.setOutput('created_at', latestRun.created_at);
          } else {
            console.log('No successful workflow runs found');
            core.setOutput('run_id', '');
          }

    - name: Download latest database
      if: steps.get_latest_db.outputs.run_id != ''
      env:
        GH_TOKEN: ${{ github.token }}
      run: |
        echo "Downloading latest database from ${{ steps.get_latest_db.outputs.workflow_name }} run ${{ steps.get_latest_db.outputs.run_id }}"
        echo "Created at: ${{ steps.get_latest_db.outputs.created_at }}"
        gh run download ${{ steps.get_latest_db.outputs.run_id }} --name posts-database --dir data/ || echo "posts-database not found"
      continue-on-error: true

    - name: Find latest images from any workflow
      id: get_latest_images
      uses: actions/github-script@v7
      with:
        script: |
          // Get runs from all workflows that might have images
          const workflows = ['fetch-posts.yml', 'daily-post.yml', 'database-status.yml'];
          const allRuns = [];
          
          for (const workflow of workflows) {
            try {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: workflow,
                branch: 'main',
                per_page: 10,
                status: 'success'
              });
              if (runs.data.workflow_runs) {
                allRuns.push(...runs.data.workflow_runs.map(run => ({
                  ...run,
                  workflow_name: workflow
                })));
              }
            } catch (error) {
              console.log(`Error fetching runs for ${workflow}:`, error.message);
            }
          }
          
          // Sort by creation time (newest first)
          allRuns.sort((a, b) => new Date(b.created_at) - new Date(a.created_at));
          
          const latestRun = allRuns[0];
          if (latestRun) {
            console.log(`Latest images source: ${latestRun.workflow_name} (run ${latestRun.id})`);
            core.setOutput('run_id', String(latestRun.id));
            core.setOutput('workflow_name', latestRun.workflow_name);
            core.setOutput('created_at', latestRun.created_at);
          } else {
            console.log('No successful workflow runs found');
            core.setOutput('run_id', '');
          }

    - name: Download latest images artifact
      if: steps.get_latest_images.outputs.run_id != ''
      env:
        GH_TOKEN: ${{ github.token }}
      run: |
        echo "Downloading images artifact from ${{ steps.get_latest_images.outputs.workflow_name }} run ${{ steps.get_latest_images.outputs.run_id }}"
        echo "Created at: ${{ steps.get_latest_images.outputs.created_at }}"
        gh run download ${{ steps.get_latest_images.outputs.run_id }} --name images --dir artifacts/images/ || echo "images artifact not found"
      continue-on-error: true
    
    - name: Prepare artifact directory
      run: |
        mkdir -p artifacts/images
    
    - name: Check image artifacts and validate database entries
      env:
        DATABASE_PATH: ./data/posts.db
        IMAGE_OUTPUT_DIR: ./artifacts/images
        PYTHONUNBUFFERED: 1
      run: |
        echo "=== IMAGE ARTIFACT DIAGNOSTICS ==="
        echo "Checking artifacts/images directory..."
        ls -la artifacts/images/ || echo "No artifacts/images directory found"
        echo ""
        echo "Checking for image files..."
        find artifacts/images -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" 2>/dev/null || echo "No image files found"
        echo ""
        echo "=== DATABASE IMAGE VALIDATION ==="
        python3 -c "
        import sqlite3
        import os
        
        if os.path.exists('./data/posts.db'):
            conn = sqlite3.connect('./data/posts.db')
            cursor = conn.cursor()
            
            # Get all messages with image URLs (join with blog_posts to get title)
            cursor.execute('''
                SELECT pm.id, pm.image_url, bp.title, pm.message_text
                FROM posted_messages pm
                JOIN blog_posts bp ON pm.blog_post_id = bp.id
                WHERE pm.image_url IS NOT NULL
                ORDER BY pm.id DESC
                LIMIT 10
            ''')
            
            messages = cursor.fetchall()
            print(f'Found {len(messages)} messages with image URLs in database')
            
            for msg in messages:
                msg_id, image_url, blog_title, message_text = msg
                print(f'\\nMessage ID: {msg_id}')
                print(f'Blog: {blog_title[:50]}...')
                print(f'Image URL: {image_url}')
                
                if os.path.isfile(image_url):
                    print('✅ Image file exists')
                else:
                    print('❌ Image file missing')
                    # Check if it's in the artifacts directory
                    if image_url.startswith('./artifacts/images/'):
                        filename = os.path.basename(image_url)
                        artifacts_path = f'./artifacts/images/{filename}'
                        if os.path.isfile(artifacts_path):
                            print(f'✅ Found in artifacts: {artifacts_path}')
                        else:
                            print(f'❌ Not found in artifacts: {artifacts_path}')
                    else:
                        print(f'❌ Path not in artifacts directory: {image_url}')
            
            conn.close()
        else:
            print('No database file found')
        "
        echo ""
        echo "=== END DIAGNOSTICS ==="
    
    - name: Post daily message
      env:
        BLOG_RSS_FEED_URL: ${{ secrets.BLOG_RSS_FEED_URL }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
        GENERATE_IMAGES: ${{ vars.GENERATE_IMAGES || '1.0' }}
        LINKEDIN_ENABLED: ${{ secrets.LINKEDIN_ENABLED || 'false' }}
        LINKEDIN_ACCESS_TOKEN: ${{ secrets.LINKEDIN_ACCESS_TOKEN }}
        LINKEDIN_USER_ID: ${{ secrets.LINKEDIN_USER_ID }}
        LINKEDIN_ORG_ID: ${{ secrets.LINKEDIN_ORG_ID }}
        LINKEDIN_POST_AS_ORG: ${{ secrets.LINKEDIN_POST_AS_ORG }}
        X_API_KEY: ${{ secrets.X_API_KEY }}
        X_API_SECRET: ${{ secrets.X_API_SECRET }}
        X_ACCESS_TOKEN: ${{ secrets.X_ACCESS_TOKEN }}
        X_ACCESS_TOKEN_SECRET: ${{ secrets.X_ACCESS_TOKEN_SECRET }}
        X_BEARER_TOKEN: ${{ secrets.X_BEARER_TOKEN }}
        DATABASE_PATH: ./data/posts.db
        POSTS_PER_BLOG: ${{ vars.POSTS_PER_BLOG || 5 }}
        IMAGE_OUTPUT_DIR: ./artifacts/images
        TEST_MODE: ${{ vars.TEST_MODE || 'false' }}
      run: |
        python main.py --post-daily
    
    - name: Cleanup used image file (post-run)
      if: success()
      run: |
        # Only delete the specific image file that was used for posting
        # The main.py script handles individual image cleanup after successful posting
        echo "Image cleanup is handled by main.py after successful posting"
    
    - name: Upload database as artifact
      uses: actions/upload-artifact@v4
      with:
        name: posts-database
        path: data/posts.db
        retention-days: 90